# SIEM Unified Pipeline Configuration
# Enhanced for Vector -> Kafka -> Pipeline -> Redis + ClickHouse -> UI flow
# Target: 500,000 events/second with exactly-once delivery

server:
  host: "0.0.0.0"
  port: 4000
  workers: 8  # Increased for high-throughput
  max_connections: 10000
  request_timeout: 30
  enable_cors: true

# Database Configuration
database:
  host: "localhost"
  port: 5432
  database: "dev"
  username: "siem_user"
  password: "siem_password"
  max_connections: 20
  min_connections: 5
  connection_timeout: 30
  idle_timeout: 600
  max_lifetime: 3600

# Rate Limiting Configuration
rate_limiting:
  enabled: true
  requests_per_second: 10000
  burst_size: 50000
  
# Data Sources Configuration
sources:
  # Enhanced Kafka source for Vector integration
  vector_kafka:
    enabled: true
    source_type:
      type: "kafka"
      topic: "vector-logs"
      brokers: ["localhost:9092"]
    config:
      format: "json"
      compression: null
      encoding: "utf-8"
      timestamp_field: "timestamp"
      timestamp_format: "%Y-%m-%dT%H:%M:%S%.3fZ"
      fields: {}
    batch_size: 1000
    buffer_size: 10000
    retry_attempts: 3
    retry_delay: 5000
    
  # Legacy syslog source (optional)
  syslog_udp:
    enabled: false
    source_type:
      type: "syslog"
      port: 514
      protocol: "udp"
    config:
      format: "syslog"
      compression: null
      encoding: "utf-8"
      timestamp_field: "timestamp"
      timestamp_format: "%Y-%m-%dT%H:%M:%S%.3fZ"
      fields: {}
    batch_size: 1000
    buffer_size: 10000
    retry_attempts: 3
    retry_delay: 5000
      
  # Legacy file source (optional)
  log_files:
    enabled: false
    source_type:
      type: "file"
      path: "/var/log/**/*.log"
      watch: true
    config:
      format: "json"
      compression: null
      encoding: "utf-8"
      timestamp_field: "timestamp"
      timestamp_format: "%Y-%m-%dT%H:%M:%S%.3fZ"
      fields: {}
    batch_size: 1000
    buffer_size: 10000
    retry_attempts: 3
    retry_delay: 5000

# Transformation Pipeline
transformations:
  # Standard normalization pipeline
  normalize_events:
    enabled: true
    parallel: false
    error_handling: "continue"
    steps:
      - type: "parse"
        parser: "json"
        config:
          field: "message"
          target_field: "parsed"
          ignore_errors: "true"
          
      - type: "map"
        field_mappings:
          "parsed.timestamp": "timestamp"
          "parsed.source_type": "source_type"
          "parsed.hostname": "hostname"
          "parsed.message": "message"
          "parsed.severity": "severity"
          "parsed.facility": "facility"
            
      - type: "normalize"
        schema: "ecs"
        strict: false
            
      - type: "enrich"
        enricher: "metadata"
        config:
          pipeline_version: "2.0.0"
          processing_timestamp: "{{now}}"
            
      - type: "enrich"
        enricher: "geoip"
        config:
          source_field: "source_ip"
          target_field: "geo"
          database_path: "/usr/share/GeoIP/GeoLite2-City.mmdb"
          
  # Security event classification
  security_classification:
    enabled: true
    parallel: false
    error_handling: "continue"
    steps:
      - type: "filter"
        condition: "message CONTAINS 'attack' OR message CONTAINS 'intrusion' OR message CONTAINS 'malware'"
        action:
          tag: "security_event"
              
      - type: "custom"
        plugin: "security_classifier"
        config:
          condition: "security_event == true"
          risk_level: "high"
          requires_investigation: "true"

# Routing Configuration
routing:
  default_destination: "redis_cache"
  load_balancing: "round_robin"
  rules:
    # High-priority security events
    - name: "security_events"
      condition: "security_event == true"
      destinations:
        - "redis_cache"
        - "clickhouse_primary"
        - "kafka_alerts"
      priority: 1
      enabled: true
        
    # Standard events
    - name: "standard_events"
      condition: "security_event != true"
      destinations:
        - "redis_cache"
        - "clickhouse_primary"
      priority: 2
      enabled: true
        
    # Error events
    - name: "error_events"
      condition: "log_level == 'error'"
      destinations:
        - "redis_cache"
        - "clickhouse_primary"
        - "kafka_errors"
      priority: 1
      enabled: true

# Data Destinations
destinations:
  # Redis cache for real-time UI streaming
  redis_cache:
    enabled: true
    destination_type:
      type: "redis"
      connection_string: "redis://localhost:6379/0"
      key_pattern: "siem:events:{timestamp}:{source}"
      ttl: 3600
    config:
      format: "json"
      compression: null
      partitioning: null
    batch_size: 1000
    flush_interval: 1000
    retry_attempts: 3
      
  # Primary ClickHouse storage
  clickhouse_primary:
    enabled: true
    destination_type:
      type: "click_house"
      connection_string: "http://localhost:8123"
      table: "siem_events"
      database: "siem"
    config:
      format: "json"
      compression: "lz4"
      partitioning: null
    batch_size: 10000
    flush_interval: 5000
    retry_attempts: 3
      
  # Kafka output for alerts
  kafka_alerts:
    enabled: true
    destination_type:
      type: "kafka"
      topic: "siem-alerts"
      brokers: ["localhost:9092"]
    config:
      format: "json"
      compression: "snappy"
      partitioning: null
    batch_size: 1000
    flush_interval: 1000
    retry_attempts: 3
      
  # Kafka output for errors
  kafka_errors:
    enabled: true
    destination_type:
      type: "kafka"
      topic: "siem-errors"
      brokers: ["localhost:9092"]
    config:
      format: "json"
      compression: "snappy"
      partitioning: null
    batch_size: 1000
    flush_interval: 1000
    retry_attempts: 3
      
  # File backup (optional)
  file_backup:
    enabled: false
    destination_type:
      type: "file"
      path: "/var/log/siem/events"
      rotation:
        size_mb: 1024
        time_hours: 24
        keep_files: 30
    config:
      format: "json"
      compression: "gzip"
    batch_size: 1000
    flush_interval: 5000
    retry_attempts: 3

# Storage Configuration
storage:
  # Data lake configuration
  data_lake:
    enabled: true
    provider: "ClickHouse"
    bucket: "siem-data-lake"
    region: "us-east-1"
    access_key: "admin"
    secret_key: "password"
    endpoint: "http://localhost:9000"
    connection_string: "tcp://localhost:9000/siem"
    
  # Hot storage (ClickHouse)
  hot_storage:
    enabled: true
    clickhouse_url: "http://localhost:8123"
    database: "siem_hot"
    retention_days: 1
    
  # Cold storage (optional)
  cold_storage:
    enabled: false
    s3_bucket: "siem-cold-storage"
    compression: "gzip"
    format: "parquet"
    
  # Retention policies
  retention:
    hot_days: 1
    warm_days: 30
    cold_days: 365
    delete_after_days: 2555  # 7 years

# Metrics and Monitoring
metrics:
  enabled: true
  port: 9092
  path: "/metrics"
  labels: {}

# Security Configuration
security:
  # TLS configuration
  tls:
    enabled: false
    cert_file: "/etc/ssl/certs/siem.crt"
    key_file: "/etc/ssl/private/siem.key"
    ca_file: "/etc/ssl/certs/ca.crt"
    
  # Authentication
  auth:
    enabled: false
    type: "jwt"
    secret_key: "your-secret-key"
    token_expiry: 3600
    
  # Rate limiting
  rate_limit:
    enabled: true
    requests_per_second: 10000  # High limit for throughput
    burst_size: 50000
    
# Performance Tuning
performance:
  # Worker configuration
  workers:
    ingestion_workers: 4
    transformation_workers: 8
    routing_workers: 4
    storage_workers: 8
    
  # Buffer configuration
  buffers:
    event_buffer_size: 100000
    batch_buffer_size: 10000
    flush_interval_ms: 1000
    
  # Memory configuration
  memory:
    max_memory_usage: "8GB"
    gc_threshold: "6GB"
    
  # Parallel processing
  parallel_processing:
    enabled: true
    worker_count: 16  # For 500k eps target
    batch_size: 1000
    batch_timeout_ms: 100

# Logging Configuration
logging:
  level: "info"
  format: "json"
  output: "stdout"
  
  # Log rotation
  rotation:
    max_size: "100MB"
    max_age: "7d"
    max_files: 10
    compress: true
    
  # Component-specific logging
  components:
    ingestion: "debug"
    transformation: "info"
    routing: "info"
    storage: "info"
    metrics: "warn"

# Health Check Configuration
health:
  enabled: true
  endpoint: "/health"
  interval: 30
  timeout: 10
  
  checks:
    - name: "kafka_connectivity"
      type: "kafka"
      config:
        brokers: "localhost:9092"
        
    - name: "redis_connectivity"
      type: "redis"
      config:
        url: "redis://localhost:6379/0"
        
    - name: "clickhouse_connectivity"
      type: "clickhouse"
      config:
        url: "tcp://localhost:9000/siem"
        
    - name: "memory_usage"
      type: "memory"
      config:
        threshold: 80  # percentage
        
    - name: "disk_usage"
      type: "disk"
      config:
        path: "/var/log/siem"
        threshold: 85  # percentage

# Development and Testing
development:
  debug_mode: false
  mock_data: false
  test_mode: false
  
  # Load testing configuration
  load_testing:
    enabled: false
    target_eps: 500000
    duration: "10m"
    ramp_up: "2m"